<title>
Applying graphics processor units to Monte Carlo dose calculation in radiation therapy.
</title>

<text>

<abstract>
We investigate the potential in using of using a graphics processor unit (GPU) for Monte-Carlo (MC)-based radiation dose calculations. The percent depth dose (PDD) of photons in a medium with known absorption and scattering coefficients is computed using a MC simulation running on both a standard CPU and a GPU. We demonstrate that the GPU's capability for massive parallel processing provides a significant acceleration in the MC calculation, and offers a significant advantage for distributed stochastic simulations on a single computer. Harnessing this potential of GPUs will help in the early adoption of MC for routine planning in a clinical environment.
</abstract>

<sections.0>
The use of onte arlo () simulations is highly desirable for dose calculation in radiation therapy as part of treatment planning or verification.  simulations represent the gold standard in radiation dose calculation since they include the real physics of the interactions of photons with materials.[1β€“4] ecause of the complexity of the physics and atomic data of the interactions, solving the photon transport equations analytically is difficult or impossible.  simulations track each photon through the medium by using random numbers to determine the occurrence of a particular interaction.[5] ince stochastic processes are involved, simulating a large number of photons is necessary to obtain reasonable statistical accuracy. his results in a computationally-expensive calculation. he standard  of a workstation or  lacks the numerical performance for this task, and may require several hours for a typical  dose calculation. n addition, manufacturers have not increased  clock speed recently, due to concerns about overheating and power consumption. ne method of addressing this issue is to exploit the highly parallelizable nature of  simulations, which permits calculations to be performed on a cluster of networked 's. or example,  simulations performed at our institution employ a cluster of 1000 dual core inux machines (2000 s), maintained and administered by a local academic supercomputing facility. owever, most clinics do not have access to such computational resources.The use of Monte Carlo (MC) simulations is highly desirable for dose calculation in radiation therapy as part of treatment planning or verification. MC simulations represent the gold standard in radiation dose calculation since they include the real physics of the interactions of photons with materials.[1β€“4] Because of the complexity of the physics and atomic data of the interactions, solving the photon transport equations analytically is difficult or impossible. MC simulations track each photon through the medium by using random numbers to determine the occurrence of a particular interaction.[5] Since stochastic processes are involved, simulating a large number of photons is necessary to obtain reasonable statistical accuracy. This results in a computationally-expensive calculation. The standard CPU of a workstation or PC lacks the numerical performance for this task, and may require several hours for a typical MC dose calculation. In addition, manufacturers have not increased CPU clock speed recently, due to concerns about overheating and power consumption. One method of addressing this issue is to exploit the highly parallelizable nature of MC simulations, which permits calculations to be performed on a cluster of networked CPU's. For example, MC simulations performed at our institution employ a cluster of 1000 dual core Linux machines (2000 CPUs), maintained and administered by a local academic supercomputing facility. However, most clinics do not have access to such computational resources.
</sections.0>

<sections.1>
Fortunately, within the last several years, the programmable  has evolved into a powerful computing device, as illustrated by igure 1.[6] ith multiple cores driven by very high memory bandwidth, 's offer low-cost computational resources for both graphics and non-graphics processing.[7β€“9] ince it was designed primarily for graphics rendering, the  architecture devotes more transistors to data processing than to data caching and flow control. ore specifically, the  is especially well suited to calculations which can be expressed as data-parallel computations and require a high ratio of arithmetic operations to memory operations.Fortunately, within the last several years, the programmable GPU has evolved into a powerful computing device, as illustrated by Figure 1.[6] With multiple cores driven by very high memory bandwidth, GPU's offer low-cost computational resources for both graphics and non-graphics processing.[7β€“9] Since it was designed primarily for graphics rendering, the GPU architecture devotes more transistors to data processing than to data caching and flow control. More specifically, the GPU is especially well suited to calculations which can be expressed as data-parallel computations and require a high ratio of arithmetic operations to memory operations.
</sections.1>

<sections.2>
We investigated the use of s for  dose calculations. uch an implementation would allow a physicist access to the computational resources of a supercomputer cluster in an inexpensive and portable machine. 's (www.nvidia.com) recently-released  programming language,  (ompute nified evice rchitecture) provided the necessary software tools.[9]We investigated the use of GPUs for MC dose calculations. Such an implementation would allow a physicist access to the computational resources of a supercomputer cluster in an inexpensive and portable machine. NVIDIA's (www.nvidia.com) recently-released GPU programming language, CUDA (Compute Unified Device Architecture) provided the necessary software tools.[9]
</sections.2>

<sections.3>
In order to compare the performances of  and  platforms for  simulations, we developed a simple photon transport  program. he code was implemented and run on both platforms, and the performances were compared. herefore, the purpose of this technical note is not to describe a physically-accurate  code, but rather to illustrate the potential advantages of  processing for this family of applications.In order to compare the performances of GPU and CPU platforms for MC simulations, we developed a simple photon transport MC program. The code was implemented and run on both platforms, and the performances were compared. Therefore, the purpose of this technical note is not to describe a physically-accurate MC code, but rather to illustrate the potential advantages of GPU processing for this family of applications.
</sections.3>

<sections.4>
The simulation is performed in the following manner.  photon is incident on a semi-infinite isotropic medium, initially normal to the surface. he photon travels a distance of up to a value, s. uring this process, the photon may undergo no interaction, or be scattered or absorbed. ince the medium is uniform, the absorption and scattering coefficients are constant throughout. o secondary-particle production is considered. f the photon is scattered, the new direction is selected randomly from a uniform distribution. hen absorption occurs, the position of the absorption is recorded, and the simulation then proceeds with the next photon. ll photons possess identical initial energy and are incident on the same point.  convolution technique can then be used to determine the dose distribution due to a beam profile of any shape. igure 2 shows the dose distribution as a function of depth resulting from a simulation of 107 histories. ote that since no secondary particle generation is considered, the curve achieves a maximum at zero depth, and the shape is different from a standard .The simulation is performed in the following manner. A photon is incident on a semi-infinite isotropic medium, initially normal to the surface. The photon travels a distance of up to a value, s. During this process, the photon may undergo no interaction, or be scattered or absorbed. Since the medium is uniform, the absorption and scattering coefficients are constant throughout. No secondary-particle production is considered. If the photon is scattered, the new direction is selected randomly from a uniform distribution. When absorption occurs, the position of the absorption is recorded, and the simulation then proceeds with the next photon. All photons possess identical initial energy and are incident on the same point. A convolution technique can then be used to determine the dose distribution due to a beam profile of any shape. Figure 2 shows the dose distribution as a function of depth resulting from a simulation of 107 histories. Note that since no secondary particle generation is considered, the curve achieves a maximum at zero depth, and the shape is different from a standard PDD.
</sections.4>

<sections.5>
We implemented the simulation on an vidia 8800 ltra  using the  language. he required random numbers were generated by the , and the history of each photon was simulated using a separate thread.  total of 4096 concurrent threads were available. he program was then implented on a standard  in the ++ language, and executed on the  for comparison with the  results comparison purposes.We implemented the simulation on an Nvidia 8800 Ultra GPU using the CUDA language. The required random numbers were generated by the GPU, and the history of each photon was simulated using a separate thread. A total of 4096 concurrent threads were available. The program was then implented on a standard PC in the C++ language, and executed on the CPU for comparison with the GPU results comparison purposes.
</sections.5>

<sections.6>
The simulation times required for the  and  execution are shown in igure 3. or a large number of histories, the  program is approximately 150 times faster, or equivalently, the  could simulate 150 times more histories than the  in a given amount of timeThe simulation times required for the CPU and GPU execution are shown in Figure 3. For a large number of histories, the GPU program is approximately 150 times faster, or equivalently, the GPU could simulate 150 times more histories than the CPU in a given amount of time
</sections.6>

<sections.7>
The potential was investigated of employing a  for  simulation in radiation therapy. t was shown that the parallel data processing capabilities of the  can be harnessed to greatly increase the number of photon histories which may be simulated in a given time. he number was up to two orders of magnitude larger than that of the  simulation. his increase, using low-cost hardware and freely-available software tools, can provide enhanced accuracy of  calculations, and make them feasible for clinical implementation.The potential was investigated of employing a GPU for MC simulation in radiation therapy. It was shown that the parallel data processing capabilities of the GPU can be harnessed to greatly increase the number of photon histories which may be simulated in a given time. The number was up to two orders of magnitude larger than that of the CPU simulation. This increase, using low-cost hardware and freely-available software tools, can provide enhanced accuracy of MC calculations, and make them feasible for clinical implementation.
</sections.7>

<sections.8>
This work was supported by an award from the oswell ark lliance oundation. his research was supported in part by grant from . ne of the authors, , would like to thank r. atthew odgorsak at oswell ark ancer nstitute for supporting the work.This work was supported by an award from the Roswell Park Alliance Foundation. This research was supported in part by grant from NYSTAR. One of the authors, MB, would like to thank Dr. Matthew Podgorsak at Roswell Park Cancer Institute for supporting the work.
</sections.8>

</text>
